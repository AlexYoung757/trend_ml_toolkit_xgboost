[xg_conf]
# DO NOT DELET OR ADD ANY PARAMETERS HERE. IF YOU HAVE TO, PLEASE REVISE THE CODE: xg_train.py

# ==========   General Parameters, see comment for each definition  ===========
# choose the booster, can be gbtree or gblinear
booster = gbtree
# Do not show the detailed information[1 Yes, 0 NO]
silent = 1


# ==========   Tree Booster Parameters   ====================
# step size shrinkage
eta = 1.0
# minimum loss reduction required to make a further partition
gamma = 0.2
# minimum sum of instance weight(hessian) needed in a child
max_delta_step = 0
colsample_bylevel = 1
lambda = 300
alpha = 0
sketch_eps = 0.03
refresh_leaf = 1



# ===============   Task Parameters   =================
# choose logistic regression loss function for binary classification
objective = binary:logistic
base_score = 0.5
seed = 0
# =============== common Parameters ====================
# 0 means do not save any model except the final round model
save_period = 0
# The path of training data
# Is the training data xg format? [1 Yes, 0 No]
xgmat = 1
data = Data/features/
label = Data/NNAI_train.txt
xgdata = Data/features/train.svmlib
eval_metric = error
ascend = 0
# eval: show the train error in each round[0 no]
eval = 1
cv = 5
#  MultiThread
nthread = 4
# the number of round to do boosting
num_round = 500


#===============  parameters need to be tuned =================
# maximum depth of a tree
max_depth = 14
subsample = 1.0
min_child_weight = 1,3,5
colsample_bytree = 0.8,0.9,1.0


